1. Demonstrate data cleaning â€“ missing values 

install.packages("tidyverse")
library(tidyverse) 
x <- sample(1:21, 20, replace = TRUE)  
y <- sample(1:10, 20, replace = TRUE) 
for(i in 1:20) 
{ 
  a <- x[i] 
  b <- y[i] 
  mtcars[a, b] = NA 
} 
which(is.na(mtcars)) 
sum(is.na(mtcars)) 
na.exclude(mtcars) 
view(mtcars) 
dispna <- apply(mtcars["disp"], 2, mean, na.rm=TRUE) 
view(dispna) 
newcars <- mtcars %>% 
  mutate(disp = ifelse(is.na(disp), dispna, disp)) 
view(newcars)



2. Implement data normalization (min-max, z-score)

arr <- c(9.5, 6.2, 8.9, 15.2, 20.0, 10.1, 5.4, 3.2, 1.0, 22.5, 10.0, 16.0) 
#min-max 
minarr <- min(arr) 
maxarr <- max(arr) 
arr2 <- arr 
for (i in 1:12){ 
  arr2[i] = round((arr[i]-minarr)/(maxarr-minarr)) 
} 
print(arr2) 
 
#z-score 
meanarr <- mean(arr) 
sdarr <- sd(arr) 
for (i in 1:12){ 
  arr2[i] = round((arr[i]-meanarr)/sdarr, 2) 
} 
print(arr2) 


3. Implement attribute subset selection for data reduction 

# Load necessary libraries
library(tidyverse)
library(leaps)

# Load the Titanic dataset and convert to a data frame
data("Titanic")
Titanic <- as.data.frame(Titanic)

# Check for NAs and remove them
sum(is.na(Titanic))
Titanic <- Titanic %>% na.omit()

# Check the dimensions of the data
dim(Titanic)

# Perform subset selection
fwd <- regsubsets(Freq ~ ., data = Titanic, nvmax = 19, method = "forward")
bwd <- regsubsets(Freq ~ ., data = Titanic, nvmax = 19, method = "backward")
full <- regsubsets(Freq ~ ., data = Titanic, nvmax = 19)

# Summarize the results
summary(fwd)
summary(bwd)
summary(full)

# Extract coefficients for the model with 3 variables
coef(fwd, 3)
coef(bwd, 3)
coef(full, 3)


4. Outlier detection

ls()
rm(day)
# Example initialization of 'day' - Replace this with your actual data loading code
day <- data.frame(
  temp = runif(100, 0, 30),
  hum = runif(100, 0, 100),
  windspeed = runif(100, 0, 15)
)
# Load necessary libraries
library(tidyverse)

# Check for NAs in the data frame
sum(is.na(day))

# Create boxplots for specified columns
boxplot(day[, c("temp", "hum", "windspeed")])

# Remove outliers for 'hum' and 'windspeed' columns
for (i in c("hum", "windspeed")) {
  data <- unlist(day[[i]])
  newData <- data[data %in% boxplot.stats(data)$out]
  data[data %in% newData] <- NA
  day[[i]] <- data
}

# Check the number of NAs in 'hum' and 'windspeed'
sum(is.na(day$hum))
sum(is.na(day$windspeed))

# Drop rows with NA values
day <- drop_na(day)

# Create boxplots again after handling outliers
boxplot(day[, c("temp", "hum", "windspeed")])

##############################################################################
day <- data.frame( temp = rnorm(100, mean = 20, sd = 5), # Normally distributed temperature hum = rnorm(100, mean = 50, sd = 15), # Normally distributed humidity windspeed = rnorm(100, mean = 10, sd = 3) # Normally distributed wind speed )
##############################################################################

ls()
rm(day)
# Load necessary libraries
library(tidyverse)

# Example initialization of 'day' with normally distributed data
set.seed(42)  # Setting a seed for reproducibility
day <- read.csv("C:/Users/ADMIN/Downloads/day.csv")

# Check for NAs in the data frame
sum(is.na(day))

# Create boxplots for specified columns
boxplot(day[, c("temp", "hum", "windspeed")])

# Remove outliers for 'hum' and 'windspeed' columns
for (i in c("hum", "windspeed")) {
  data <- unlist(day[[i]])
  newData <- data[data %in% boxplot.stats(data)$out]
  data[data %in% newData] <- NA
  day[[i]] <- data
}

# Check the number of NAs in 'hum' and 'windspeed'
sum(is.na(day$hum))
sum(is.na(day$windspeed))

# Drop rows with NA values
day <- drop_na(day)

# Create boxplots again after handling outliers
boxplot(day[, c("temp", "hum", "windspeed")])





5.Perform analytics on any standard data set 

install.packages("titanic")
# Load necessary libraries
library(tidyverse)
#library(titanic)

# Load the Titanic dataset
#data("titanic_train")
titanic <- read.csv("C:/Users/ADMIN/Downloads/titanic.csv")

# Display the first few rows of the dataset
head(titanic)

# Display the class of each column
sapply(titanic, class)

# Convert 'Sex' and 'Survived' columns to factors
titanic$Sex <- as.factor(titanic$Sex)
titanic$Survived <- as.factor(titanic$Survived)

# Display summary statistics
summary(titanic)

# Remove rows with any NAs
dropnull_titanic <- titanic[rowSums(is.na(titanic)) <= 0, ]

# Create subsets based on survival
survivedList <- dropnull_titanic[dropnull_titanic$Survived == 1, ]
notSurvivedList <- dropnull_titanic[dropnull_titanic$Survived == 0, ]

# Create a table and labels for the pie chart
mytable <- table(titanic$Survived)
lbls <- paste(names(mytable), "\n", mytable, sep = "")

# Create a pie chart
pie(
  mytable,
  labels = lbls,
  main = "Pie Chart of Survival"
)

# Create a histogram of ages
hist(titanic$Age, xlab = "Age", ylab = "Frequency", main = "Age Distribution")

# Create a bar plot of gender distribution among those who did not survive
barplot(table(notSurvivedList$Sex), xlab = "Gender", ylab = "Frequency", main = "Gender Distribution among Non-Survivors")

# Create a density plot for fares of those who survived
fare_density <- density(survivedList$Fare)
plot(fare_density, type = "n", main = "Density Plot of Fares (Survivors)")
polygon(fare_density, col = "lightgray", border = "gray")

# Create a boxplot of fares
boxplot(titanic$Fare, main = "Boxplot of Fares", ylab = "Fare")


6. Implement linear regression
install.packages("tidyverse")
install.packages("caTools")

# Load necessary libraries
library(tidyverse)
library(caTools)

# Create the data frame
data <- data.frame(
  Years_Exp = c(1.1, 1.3, 1.5, 2.0, 2.2, 2.9, 3.0, 3.2, 3.2, 3.7),
  Salary = c(39343.00, 46205.00, 37731.00, 43525.00,
             39891.00, 56642.00, 60150.00, 54445.00, 64445.00, 57189.00)
)

# Split the data into training and test sets
set.seed(42)
split = sample.split(data$Salary, SplitRatio = 0.7)
train = subset(data, split == TRUE)
test = subset(data, split == FALSE)

# Fit the linear regression model
lm.r = lm(formula = Salary ~ Years_Exp, data = train)

# Print the coefficients
print(coef(lm.r))

# Plot the data and the regression line
ggplot() + 
  geom_point(aes(x = train$Years_Exp, y = train$Salary), col = 'red') + 
  geom_line(aes(x = train$Years_Exp, y = predict(lm.r, newdata = train)), col = "blue") + ggtitle("Salary vs Experience")

7.Implement logistic regression


library(tidyverse)
library(ROCR)
library(caTools)

# Load and inspect the dataset


view(mtcars)

# Split the dataset into training and testing sets
set.seed(42)  # Set seed for reproducibility
split <- sample.split(mtcars$vs, SplitRatio = 0.8)
train <- subset(mtcars, split == TRUE)
test <- subset(mtcars, split == FALSE)

# Fit the logistic regression model
logistic_model <- glm(vs ~ wt + disp, data = train, family = binomial)
summary(logistic_model)

# Make predictions on the test set
predict_reg <- predict(logistic_model, test, type = "response")

# Convert probabilities to binary outcomes
predict_reg <- ifelse(predict_reg > 0.5, 1, 0)

# Create a confusion matrix
table(test$vs, predict_reg)

# Calculate and print the misclassification error
missing_classerr <- mean(predict_reg != test$vs)
print(missing_classerr)
print(paste("accuracy = ", (1 - missing_classerr)))

# Plot logistic regression curve using ggplot2
ggplot(mtcars, aes(x = wt + disp, y = vs)) + 
  geom_point(alpha = 0.5) + 
  stat_smooth(method = "glm", se = FALSE, method.args = list(family = binomial), col = "red") + 
  ggtitle("Logistic Regression Curve")

# ROC Curve
ROCPred <- prediction(predict_reg, test$vs)
ROCPer <- performance(ROCPred, measure = "tpr", x.measure = "fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
print(auc)

plot(ROCPer, colorize = TRUE, print.cutoffs.at = seq(0.1, by = 0.1), main = "ROC Curve")
abline(a = 0, b = 1)

auc <- round(auc, 4)
legend(.6, .4, auc, title = "AUC", cex = 1)


8. Construct decision tree for weather data set

set.seed(42) 
weatherdata <- data.frame( 
  cloud3pm = sample(1:10, 1000, replace = TRUE), 
  pressure3pm = sample(1000:1025, 1000, replace = TRUE), 
  humidity3pm = sample(40:100, 1000, replace = TRUE), 
  temp3pm = sample(10:35, 1000, replace = TRUE), 
  wind3pm = sample(0:50, 1000, replace = TRUE), 
  RainTomorrow = factor(sample(c('No', 'Yes'), 1000, replace = TRUE)) 
) 
 
table(weatherdata$RainTomorrow) 
repeat { 
  sample <- sample(c(TRUE, FALSE), nrow(weatherdata), replace = TRUE, prob = c(0.8, 0.2)) 
  train <- weatherdata[sample, ] 
  test <- weatherdata[!sample, ] 
  if (length(unique(test$RainTomorrow)) == 2) break 
} 
 
library(partykit) 
model <- ctree(RainTomorrow ~ ., data = train, control = ctree_control(minsplit = 5, minbucket = 
5, maxdepth = 10, mincriterion = 0.7)) 
plot(model, type = 'simple') 
 
predict_model <- predict(model, test) 
mat <- table(test$RainTomorrow, predict_model) 
accuracy <- sum(diag(mat)) / sum(mat) 
sprintf("Accuracy: %f ", accuracy * 100)


########################################
library(tidyverse)
library(partykit)
library(caTools)

# download & load dataset:
#https://www.kaggle.com/datasets/petalme/seattle-weather-prediction-dataset

weatherdata <- read.csv("C:/Users/ADMIN/Downloads/seattle-weather.csv")

# Inspect the dataset
head(weatherdata)
str(weatherdata)
# Convert 'weather' to a factor for classification
weatherdata$weather <- as.factor(weatherdata$weather)

# Split the dataset into training and testing sets (80-20 split)
split=sample.split(weatherdata$weather, SplitRatio=0.8)
train=subset(weatherdata, split==TRUE)
test=subset(weatherdata, split==FALSE)
 
# Train a decision tree model to predict 'weather'
model <- ctree(weather ~ precipitation + temp_max + temp_min + wind, data = train)

# Plot the decision tree
plot(model)
# Make predictions on the test set
predict_model <- predict(model, test)

# Generate a confusion matrix to evaluate model performance
mat <- table(test$weather, predict_model)
print(mat)

# Calculate the accuracy of the model
accuracy <- sum(diag(mat)) / sum(mat)
print(paste("Accuracy:", accuracy))



9.Analyse Time-Series Data


# Load necessary libraries
library(tidyverse)
library(lubridate)
library(forecast)

# Define the data
positiveCases <- c(580, 7813, 28266, 59287, 75700, 87820, 95314, 126214, 471497, 936851, 1508725, 2072113)
deaths <- c(17, 270, 565, 1261, 2126, 2800, 3285, 4628, 8951, 21283, 47210, 88480)

# Check if lengths are equal
if(length(positiveCases) != length(deaths)) {
  stop("Vectors 'positiveCases' and 'deaths' must have the same length")
}

# Create time series data
mts <- ts(cbind(positiveCases, deaths), start = decimal_date(ymd("2021-01-22")), frequency = 365.25/7)

# Plot the multi-time series data
plot(mts, xlab = "Weekly Data", main = "COVID-19 Cases", col.main = "darkgreen")

# Create time series data for positive cases
mts1 <- ts(positiveCases, start = decimal_date(ymd("2021-01-22")), frequency = 365.25/7)

# Fit ARIMA model and forecast
fit <- auto.arima(mts1)
forecast_fit <- forecast(fit, 5)

# Plot the forecast
plot(forecast_fit, xlab = "Weekly Data", ylab = "Positive Cases", main = "COVID-19 Cases", col.main = "green")


10. Work on any Data Visualization tools

View(airquality) 
barplot(airquality$Ozone, main = 'Ozone Concentration in Air', xlab = 'Ozone Levels', horiz = 
TRUE) 
hist(airquality$Temp, main = "La Guardia Airport's Maximum Temperature(Daily)", xlab = 
"Temperature(Fahrenheit)", xlim = c(50,125), col = "yellow", freq = TRUE) 
boxplot(airquality[,0:4], main = "Box Plots for Air Quality Parameters") 
plot(airquality$Ozone, airquality$Month, main = "Scatterplot Example", xlab = "Ozone 
Concentration in ppb", ylab = "Month of Observation", pch = 19) 
data <- matrix(rnorm(50, 0, 5), nrow = 5, ncol = 5) 
colnames(data) <- paste0("col", 1:5) 
rownames(data) <- paste0("row", 1:5) 
heatmap(data)

